{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"ee-notebook-buttons\" align=\"left\">\n",
    "    <td><a target=\"_blank\"  href=\"https://github.com/NINAnor/urban-treeDetection\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" style=\"filter: invert(100%)\"/> View source on GitHub</a></td>\n",
    "    <td><a target=\"_blank\"  href=\"https://drive.google.com/drive/folders/1mEQBfa-tVViVWFt27XzUP4Wr19u1iuZm\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /> Run in Google Colab</a></td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Tree Detection | Extrapolating Ecosystem Service Values\n",
    "\n",
    "**Author:** Willeke A'Campo\n",
    "\n",
    "**Description:** Script to clean the output i-Tree Eco dataset and extrapolate the Ecosystem Service Values to all trees in the municipality. The regression is trained on the in situ tree dataset from the municipality or if not availabe on Oslo's tree in situ tree data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import geopandas as gpd\n",
    "from scipy import stats\n",
    "\n",
    "# from splot.esda import moran_scatterplot, plot_moran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  py-packages\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# import local packages\n",
    "from src import MUNICIPALITY, RAW_PATH, INTERIM_PATH\n",
    "from src import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RAW_PATH)\n",
    "print(MUNICIPALITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure seaborn plot\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",  # paper, notebook, talk, and poster\n",
    "    style=\"darkgrid\",  # darkgrid, whitegrid, dark, white, and ticks\n",
    "    palette=\"dark\",  # deep, muted, bright, pastel, dark, and colorblind\n",
    "    font=\"sans-serif\",  # font family: serif, sans-serif, cursive, fantasy, and monospace\n",
    "    font_scale=1,  # 1 = default font size (12pt)\n",
    "    color_codes=True,  # False = don't color code from current palette\n",
    "    rc=None,  # dict for additional settings (e.g. axes.labelsize=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure logger\n",
    "logger.setup_logger(logfile=False)\n",
    "logging.info(f\"Start preparing the tree database of {MUNICIPALITY}.\")\n",
    "\n",
    "# print global project variables\n",
    "print(f\"interim_data_path: {INTERIM_PATH}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reference Data Oslo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Excel file into a lookup dictionairy\n",
    "excel_path = os.path.join(INTERIM_PATH, \"kristiansand_itree_eco.xlsx\")\n",
    "print(excel_path)\n",
    "\n",
    "workbook = pd.ExcelFile(excel_path)\n",
    "sheet_names = workbook.sheet_names\n",
    "print(f\"workbook sheet names: {sheet_names}\")\n",
    "\n",
    "# import Oslo's reference dataset\n",
    "df_raw = pd.read_excel(excel_path, sheet_name=\"oslo_reference_data\")\n",
    "\n",
    "# import metadata\n",
    "df_metadata = pd.read_excel(excel_path, sheet_name=\"metadata\")\n",
    "df_metadata = df_metadata[[\"urban-treeDetection_colnames\", \"python_colnames\", \"dtype\"]]\n",
    "\n",
    "# import genus list\n",
    "df_genus = pd.read_excel(excel_path, sheet_name=\"oslo_unique_genus\")\n",
    "genus_bins = df_genus[\"genus\"].tolist()\n",
    "\n",
    "print(\"Raw data information: Oslo Reference Data\")\n",
    "display(df_raw.head())\n",
    "print(\"Rows in dataframe: \", len(df_raw))\n",
    "print(f\"\\nunique genus: {genus_bins}\")\n",
    "print(\"\\nMetadata information: \")\n",
    "display(df_metadata.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset\n",
    "- remove rows with missing values\n",
    "- remove rows with negative values (not done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df = df_raw.dropna()\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "display(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rename columns \n",
    "- trim column names\n",
    "- set dtype of columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim and rename columns using metadata\n",
    "col_name_mapping = dict(\n",
    "    zip(df_metadata[\"urban-treeDetection_colnames\"], df_metadata[\"python_colnames\"])\n",
    ")\n",
    "df.columns = df.columns.str.strip()\n",
    "df.rename(columns=col_name_mapping, inplace=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# update column dtypes using metadata\n",
    "dtype_mapping = dict(zip(df_metadata[\"python_colnames\"], df_metadata[\"dtype\"]))\n",
    "\n",
    "# remove all values from dtype_mapping if they are not in df.columns\n",
    "dtype_mapping = {\n",
    "    key: dtype_mapping[key] for key in dtype_mapping.keys() if key in df.columns\n",
    "}\n",
    "\n",
    "# convert column dtypes using dtype_mapping\n",
    "for key in dtype_mapping.keys():\n",
    "    # print(f\"column: {key}\\t current dtype: {df[key].dtypes} \\ttarget dtype: {dtype_mapping[key]}\")\n",
    "    # if the current dtype is not the target dtype, convert the column\n",
    "    if df[key].dtypes != dtype_mapping[key]:\n",
    "        try:\n",
    "            if dtype_mapping[key] == \"int\":\n",
    "                df[key] = df[key].astype(int)\n",
    "                print(f\"column: {key} converted to {df[key].dtypes}\")\n",
    "            elif dtype_mapping[key] == \"float64\":\n",
    "                df[key] = df[key].astype(float)\n",
    "                print(f\"column: {key} converted to {df[key].dtypes}\")\n",
    "            elif dtype_mapping[key] == \"object\":\n",
    "                df.astype({key: \"object\"}).dtypes\n",
    "                print(f\"column: {key} converted to {df[key].dtypes}\")\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"column: {key} ({df[key].dtypes}) could not be converted to {dtype_mapping[key]}\"\n",
    "            )\n",
    "            pass\n",
    "    else:\n",
    "        print(\n",
    "            f\"column: {key} ({df[key].dtypes}) has already dtype: {dtype_mapping[key]}\"\n",
    "        )\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean Reference dataset (Kristiansand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import target dataset\n",
    "excel_path = os.path.join(INTERIM_PATH, \"kristiansand_itree_eco.xlsx\")\n",
    "df_target = pd.read_excel(excel_path, sheet_name=\"kristiansand_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values\n",
    "df_target = df_target.dropna()\n",
    "# drop rows with missing values\n",
    "df_target = df_target.dropna()\n",
    "display(df_target.head())\n",
    "print(df_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = df.columns\n",
    "\n",
    "# drop columns that are in the target dataset\n",
    "col_list = [col for col in col_list if col not in df_target.columns]\n",
    "print(col_list)\n",
    "\n",
    "# add col list as columns to target_df\n",
    "df_target = pd.concat([df_target, pd.DataFrame(columns=col_list)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target.columns = df_target.columns.str.strip()\n",
    "\n",
    "# remove all values from dtype_mapping if they are not in df.columns\n",
    "dtype_mapping = {\n",
    "    key: dtype_mapping[key] for key in dtype_mapping.keys() if key in df_target.columns\n",
    "}\n",
    "\n",
    "# convert column dtypes using dtype_mapping\n",
    "for key in dtype_mapping.keys():\n",
    "    # print(f\"column: {key}\\t current dtype: {df[key].dtypes} \\ttarget dtype: {dtype_mapping[key]}\")\n",
    "    # if the current dtype is not the target dtype, convert the column\n",
    "    if df_target[key].dtypes != dtype_mapping[key]:\n",
    "        try:\n",
    "            if dtype_mapping[key] == \"int\":\n",
    "                df_target[key] = df_target[key].astype(int)\n",
    "                print(f\"column: {key} converted to {df_target[key].dtypes}\")\n",
    "            elif dtype_mapping[key] == \"float64\":\n",
    "                df_target[key] = df_target[key].astype(float)\n",
    "                print(f\"column: {key} converted to {df_target[key].dtypes}\")\n",
    "            elif dtype_mapping[key] == \"object\":\n",
    "                df_target.astype({key: \"object\"}).dtypes\n",
    "                print(f\"column: {key} converted to {df_target[key].dtypes}\")\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"column: {key} ({df_target[key].dtypes}) could not be converted to {dtype_mapping[key]}\"\n",
    "            )\n",
    "            pass\n",
    "    else:\n",
    "        print(\n",
    "            f\"column: {key} ({df_target[key].dtypes}) has already dtype: {dtype_mapping[key]}\"\n",
    "        )\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation analysis Oslo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with name 'id' and 'genus'\n",
    "df_corr = df.drop(\n",
    "    [\"tree_id\", \"crown_id\", \"dbh\", \"dbh_in_situ\", \"sp_in_situ\", \"lon\", \"lat\"], axis=1\n",
    ")\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "ax.set_title(\"Correlation Matirx\", fontweight=\"ultralight\", fontsize=25)\n",
    "plt.xticks(rotation=45, fontweight=\"ultralight\", fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- select columns to keep for modelling\n",
    "- round values for pred and response var to 3 decimals (ensure that X,Y columns are not rounded!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.columns)\n",
    "col_id = [\"tree_id\", \"crown_id\"]\n",
    "col_coord = [\"lon\", \"lat\"]\n",
    "predictors = [\"tree_height\", \"crown_area\", \"pollution_zone\", \"genus\"]\n",
    "response_vars = [\n",
    "    \"carbon_storage\",\n",
    "    \"carbon_seq\",\n",
    "    \"runoff\",\n",
    "    \"polution_no2\",\n",
    "    \"polution_so2\",\n",
    "    \"polution_pm25\",\n",
    "    \"polution_co\",\n",
    "    \"polution_o3\",\n",
    "    \"totben_cap\",\n",
    "]\n",
    "\n",
    "col_to_keep = col_id + predictors + response_vars + col_coord\n",
    "print(col_to_keep)\n",
    "df = df[col_to_keep]\n",
    "\n",
    "# round response and predictor values to 2 decimals\n",
    "df[predictors + response_vars] = df[predictors + response_vars].round(2)\n",
    "display(df.head())\n",
    "\n",
    "\n",
    "df_target = df_target[col_to_keep]\n",
    "df_target[predictors + response_vars] = df_target[predictors + response_vars].round(2)\n",
    "display(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a histogram for genus distribution\n",
    "df_genus = df.groupby(\"genus\").count()\n",
    "df_genus = df_genus[[\"tree_id\"]]\n",
    "df_genus.rename(columns={\"tree_id\": \"count\"}, inplace=True)\n",
    "df_genus.sort_values(by=[\"count\"], inplace=True, ascending=False)\n",
    "df_genus.reset_index(inplace=True)\n",
    "\n",
    "# probability distribution of genus in percent\n",
    "df_genus[\"probability\"] = df_genus[\"count\"] / df_genus[\"count\"].sum()\n",
    "df_genus[\"prob_percentage\"] = round(df_genus[\"probability\"] * 100)\n",
    "display(df_genus.head())\n",
    "\n",
    "df_genus[\"probability\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "fig, ax = plt.subplots(figsize=(30, 5))\n",
    "sns.barplot(x=\"genus\", y=\"prob_percentage\", data=df_genus, ax=ax, color=\"darkgreen\")\n",
    "ax.set_title(\n",
    "    \"Tree Genus Probability Distribution in Oslo\", fontweight=\"ultralight\", fontsize=20\n",
    ")\n",
    "ax.set_xlabel(\"Genus\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Probability (%)\", fontweight=\"bold\")\n",
    "plt.xticks(rotation=45, fontweight=\"ultralight\")\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Target Dataset with Genus values using the probability distribution of Oslo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fil no data values in test_df['genus'] by using the probability distribution of df['genus']\n",
    "df_target[\"genus\"] = df_target[\"genus\"].apply(\n",
    "    lambda x: np.random.choice(df_genus[\"genus\"], p=df_genus[\"probability\"])\n",
    "    if pd.isnull(x)\n",
    "    else x\n",
    ")\n",
    "\n",
    "# print the probability distribution of target_data['genus']\n",
    "target_data_genus = df_target.groupby(\"genus\").count()\n",
    "target_data_genus = target_data_genus[[\"tree_id\"]]\n",
    "target_data_genus.rename(columns={\"tree_id\": \"count\"}, inplace=True)\n",
    "target_data_genus.sort_values(by=[\"count\"], inplace=True, ascending=False)\n",
    "target_data_genus.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability distribution of genus in percent\n",
    "target_data_genus[\"probability\"] = (\n",
    "    target_data_genus[\"count\"] / target_data_genus[\"count\"].sum()\n",
    ")\n",
    "target_data_genus[\"prob_percentage\"] = round(target_data_genus[\"probability\"] * 100)\n",
    "display(target_data_genus.head())\n",
    "\n",
    "# plot histogram\n",
    "fig, ax = plt.subplots(figsize=(30, 5))\n",
    "sns.barplot(\n",
    "    x=\"genus\", y=\"prob_percentage\", data=target_data_genus, ax=ax, color=\"darkgreen\"\n",
    ")\n",
    "ax.set_title(\"Test Tree Species Distribution\", fontweight=\"ultralight\", fontsize=20)\n",
    "ax.set_xlabel(\"Genus\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Probability (%)\", fontweight=\"bold\")\n",
    "plt.xticks(rotation=45, fontweight=\"ultralight\")\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_target.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-weighted Regression with encoded genus info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for species\n",
    "df[\"genus\"] = df[\"genus\"].astype(\"category\")\n",
    "\n",
    "# encode genus for regression\n",
    "genus_encoded = pd.get_dummies(df[\"genus\"], prefix=\"genus\")\n",
    "\n",
    "# get genus encoded column names\n",
    "genus_encoded_cols = list(genus_encoded.columns)\n",
    "# replace genus_ wiht \"\" in column names\n",
    "genus_encoded_cols = [x.replace(\"genus_\", \"\") for x in genus_encoded_cols]\n",
    "genus_encoded_cols = [x.lower() for x in genus_encoded_cols]\n",
    "\n",
    "# replace col names with formatted col names\n",
    "genus_encoded.columns = genus_encoded_cols\n",
    "\n",
    "df = pd.concat([df, genus_encoded], axis=1)\n",
    "print(genus_encoded_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_genus_encoded = pd.get_dummies(df_target[\"genus\"])\n",
    "target_genus_encoded_cols = list(target_genus_encoded.columns)\n",
    "target_genus_encoded_cols = [x.lower() for x in target_genus_encoded_cols]\n",
    "\n",
    "# replace col names with formatted col names\n",
    "target_genus_encoded.columns = target_genus_encoded_cols\n",
    "\n",
    "df_target = pd.concat([df_target, target_genus_encoded], axis=1)\n",
    "display(df_target.head())\n",
    "print(target_genus_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that df and df_target contain the same column names\n",
    "same_columns = df.columns.equals(df_target.columns)\n",
    "print(same_columns)\n",
    "\n",
    "# drop columns that are not in df_target\n",
    "if not same_columns:\n",
    "    df = df.drop([col for col in df.columns if col not in df_target.columns], axis=1)\n",
    "    print(df.columns)\n",
    "    print(df_target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features (X) and the target variable (y) from the DataFrame\n",
    "predictors = [\"tree_height\", \"crown_area\", \"pollution_zone\"] + genus_encoded_cols\n",
    "response_vars = [\n",
    "    \"carbon_storage\",\n",
    "    \"carbon_seq\",\n",
    "    \"runoff\",\n",
    "    \"polution_no2\",\n",
    "    \"polution_so2\",\n",
    "    \"polution_pm25\",\n",
    "    \"polution_co\",\n",
    "    \"polution_o3\",\n",
    "    \"totben_cap\",\n",
    "]\n",
    "\n",
    "for var in response_vars:\n",
    "    y = df[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genus encoded columns is all columns that are not within list \n",
    "no_genus = ['tree_id', 'crown_id', 'tree_height', 'crown_area', 'pollution_zone',\n",
    "       'genus', 'carbon_storage', 'carbon_seq', 'runoff', 'polution_no2',\n",
    "       'polution_so2', 'polution_pm25', 'polution_co', 'polution_o3',\n",
    "       'totben_cap', 'lon', 'lat']\n",
    "genus_encoded = [col for col in df.columns if col not in no_genus]\n",
    "print(genus_encoded)\n",
    "\n",
    "predictors = [\"tree_height\", \"crown_area\", \"pollution_zone\"] + genus_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictors]\n",
    "X_target = df_target[predictors]\n",
    "print(X_target)\n",
    "seed = 42\n",
    "test_size = 0.2\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "# TODO store model in dictionairy {model_name:model}\n",
    "# TODO store model results in dataframe df_results dict. {model_name:df_results}\n",
    "\n",
    "# loop over response variables and calc. linear regression\n",
    "for var in response_vars:\n",
    "    y = df[var]\n",
    "\n",
    "    # SPLIT DATA\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    # TRAIN MODEL\n",
    "    # Create and fit the regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    b0 = round(model.intercept_, 1)\n",
    "    b1 = round(model.coef_[0], 1)\n",
    "    b2 = round(model.coef_[1], 1)\n",
    "    b3 = round(model.coef_[2], 1)\n",
    "    b4 = round(model.coef_[3], 1)\n",
    "    y = var\n",
    "    x1 = predictors[0]\n",
    "    x2 = predictors[1]\n",
    "    x3 = predictors[2]\n",
    "    x4 = predictors[3]\n",
    "\n",
    "    model_equation = f\"y = {b0} + {b1}*{x1} + {b2}*{x2} + {b3}*{x3} + {b4}*{x4} + ...\"\n",
    "\n",
    "    # Make predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # EVALUATE MODEL\n",
    "    # store model results in dataframe df_results\n",
    "    r2_train = round(r2_score(y_train, y_train_pred), 2)\n",
    "    r2_test = round(r2_score(y_test, y_test_pred), 2)\n",
    "    r2_train = round(r2_train, 2)\n",
    "    rmse_train = round(np.sqrt(mean_squared_error(y_train, y_train_pred)), 2)\n",
    "\n",
    "    df_results = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"model_name\",\n",
    "            \"response_var\",\n",
    "            \"equation\",\n",
    "            \"train_rmse\",\n",
    "            \"test_rmse\",\n",
    "            \"train_r2\",\n",
    "            \"test_r2\",\n",
    "        ]\n",
    "    )\n",
    "    df_results[\"model_name\"] = f\"model_{var}\"\n",
    "    df_results[\"response_var\"] = var\n",
    "    df_results[\"equation\"] = model_equation\n",
    "    df_results[\"train_rmse\"] = rmse_train\n",
    "    df_results[\"test_rmse\"] = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    df_results[\"train_r2\"] = r2_train\n",
    "    df_results[\"test_r2\"] = r2_test\n",
    "\n",
    "    # append results dataframe to dict\n",
    "    dict_results[f\"model_{var}\"] = df_results\n",
    "\n",
    "    # DEPLOY MODEL\n",
    "    # Apply the regression model to the df_target dataset\n",
    "    y_target_pred = model.predict(X_target)\n",
    "\n",
    "    # Fill the response columns in df_target with the predicted values\n",
    "    df_target[var] = y_target_pred\n",
    "\n",
    "    # DISPLAY RESULTS\n",
    "    print(f\"Model: {var}\")\n",
    "    print(f\"Equation: {model_equation}\")\n",
    "    display(df_target.head())\n",
    "\n",
    "    # plot results and save plot to output folder\n",
    "    plt.clf()  # clear figure\n",
    "\n",
    "    # Regression plot\n",
    "    sns.jointplot(\n",
    "        x=y_test,\n",
    "        y=y_test_pred,\n",
    "        kind=\"reg\",\n",
    "        truncate=False,\n",
    "        # xlim=(0, 60), ylim=(0, 12),\n",
    "        color=\"#427360\",\n",
    "        height=7,\n",
    "    )\n",
    "    plt.xlabel(f\"actual {var}\")\n",
    "    plt.ylabel(f\"predicted {var}\")\n",
    "    plt.figtext(\n",
    "        0,\n",
    "        -0.05,\n",
    "        f\"R2: {r2_train}   RMSE: {rmse_train}\\n {model_equation}\",\n",
    "        ha=\"left\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    # save plot to output folder\n",
    "    # output_path = os.path.join(INTERIM_PATH, f'linear_reg_{var}.png')\n",
    "    # plt.savefig(output_path, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_target.head())\n",
    "\n",
    "# round values\n",
    "df_target[predictors + response_vars] = round(df_target[predictors + response_vars], 3)\n",
    "\n",
    "# drop genus cols\n",
    "drop_cols = target_genus_encoded.columns\n",
    "\n",
    "\n",
    "df_target.drop(drop_cols, axis=1, inplace=True)\n",
    "display(df_target.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the results in df_target\n",
    "csv_path = os.path.join(INTERIM_PATH, \"kristiansand_extrapolation_from_oslo_results.csv\")\n",
    "df_target.to_csv(csv_path, index=False, sep=\";\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACKUP | Non-weighted Regression without genus\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + b3*x3 + ...\n",
    "\n",
    "where:\n",
    "\n",
    "y is the dependent variable,\n",
    "x1, x2, x3, ... are the independent variables,\n",
    "b0 is the y-intercept of the regression line, and\n",
    "b1, b2, b3, ... are the coefficients for each independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features (X) and the target variable (y) from the DataFrame\n",
    "print(df.columns)\n",
    "\n",
    "predictors = [\"tree_height\", \"crown_area\", \"pollution_zone\"]\n",
    "response_vars = [\n",
    "    \"carbon_storage\",\n",
    "    \"carbon_seq\",\n",
    "    \"runoff\",\n",
    "    \"polution_no2\",\n",
    "    \"polution_so2\",\n",
    "    \"polution_pm25\",\n",
    "    \"polution_co\",\n",
    "    \"polution_o3\",\n",
    "    \"totben_cap\",\n",
    "]\n",
    "\n",
    "for var in response_vars:\n",
    "    y = df[var]\n",
    "\n",
    "X = df[predictors]\n",
    "seed = 42\n",
    "test_size = 0.2\n",
    "\n",
    "dict_results = {}\n",
    "\n",
    "# TODO store model in dictionairy {model_name:model}\n",
    "# TODO store model results in dataframe df_results dict. {model_name:df_results}\n",
    "\n",
    "# loop over response variables and calc. linear regression\n",
    "for var in response_vars:\n",
    "    y = df[var]\n",
    "    # display(X.head())\n",
    "    # display(y.head())\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Create and fit the regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    b0 = round(model.intercept_, 1)\n",
    "    b1 = round(model.coef_[0], 1)\n",
    "    b2 = round(model.coef_[1], 1)\n",
    "    b3 = round(model.coef_[2], 1)\n",
    "    y = var\n",
    "    x1 = predictors[0]\n",
    "    x2 = predictors[1]\n",
    "    x3 = predictors[2]\n",
    "    model_equation = f\"y = {b0} + {b1}*{x1} + {b2}*{x2} + {b3}*{x3}\"\n",
    "\n",
    "    # model_equation = f'y = {model.intercept_} + {model.coef_[0]}*x1 + \\\n",
    "    #      {model.coef_[1]}*x2 + {model.coef_[2]}*x3'\n",
    "\n",
    "    # Make predictions on the training and testing sets\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    # store model results in dataframe df_results\n",
    "    # r2 train round to 2\n",
    "\n",
    "    r2_train = round(r2_score(y_train, y_train_pred), 2)\n",
    "    r2_test = round(r2_score(y_test, y_test_pred), 2)\n",
    "    r2_train = round(r2_train, 2)\n",
    "    df_results = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"model_name\",\n",
    "            \"response_var\",\n",
    "            \"equation\",\n",
    "            \"train_rmse\",\n",
    "            \"test_rmse\",\n",
    "            \"train_r2\",\n",
    "            \"test_r2\",\n",
    "        ]\n",
    "    )\n",
    "    df_results[\"model_name\"] = f\"model_{var}\"\n",
    "    df_results[\"response_var\"] = var\n",
    "    df_results[\"equation\"] = model_equation\n",
    "    df_results[\"train_rmse\"] = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    df_results[\"test_rmse\"] = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    df_results[\"train_r2\"] = r2_train\n",
    "    df_results[\"test_r2\"] = r2_test\n",
    "\n",
    "    # append results dataframe to dict\n",
    "    dict_results[f\"model_{var}\"] = df_results\n",
    "\n",
    "    # plot results and save plot to output folder\n",
    "    fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
    "    # Add the regression line of best fit\n",
    "    plt.plot(\n",
    "        [y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"--k\", label=\"Ideal\"\n",
    "    )\n",
    "    plt.plot(y_test, y_test_pred, \"bo\", label=\"Predictions\")\n",
    "    plt.legend()\n",
    "    plt.scatter(y_test, y_test_pred)\n",
    "    plt.xlabel(f\"Actual {var}\")\n",
    "    plt.ylabel(f\"Predicted {var}\")\n",
    "    plt.title(f\"Actual vs Predicted {var}\")\n",
    "    # # print R2 and model equation under the plot\n",
    "    # Add text under the figure in italic\n",
    "    plt.figtext(0, -0.05, f\"R2: {r2_train}\\n {model_equation}\", ha=\"left\", fontsize=8)\n",
    "\n",
    "    # plt.text(f\"R2: {df_results['test_r2'][0]:.2f}\", fontsize=12)\n",
    "    # plt.text(f\"Equation: {model_equation}\", fontsize=12)\n",
    "\n",
    "    # # save plot to output folder\n",
    "    # output_path = os.path.join(INTERIM_PATH, f'linear_reg_{var}.png')\n",
    "    # #plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # FIGURE 2\n",
    "    sns.jointplot(\n",
    "        x=y_test,\n",
    "        y=y_test_pred,\n",
    "        kind=\"reg\",\n",
    "        truncate=False,\n",
    "        # xlim=(0, 60), ylim=(0, 12),\n",
    "        color=\"#427360\",\n",
    "        height=7,\n",
    "    )\n",
    "    plt.xlabel(f\"Actual {var}\")\n",
    "    plt.ylabel(f\"Predicted {var}\")\n",
    "    # # print R2 and model equation under the plot\n",
    "    # Add text under the figure in italic\n",
    "    plt.figtext(0, -0.05, f\"R2: {r2_train}\\n {model_equation}\", ha=\"left\", fontsize=8)\n",
    "    plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urban-treeDetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
